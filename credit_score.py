# -*- coding: utf-8 -*-
"""Credit_score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18BWKoshj9DXRpjIypjB6igWTgLjPUKlb

**MODELING WITH TARGET SET BY THE BOUNDARIES OF FICO SCORE**
"""

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import pandas as pd
import numpy as np
from typing import Tuple, Dict

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.calibration import CalibratedClassifierCV

from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix, mean_squared_error, mean_absolute_error
)

CSV_PATH    = "/content/final_date_version(Sheet1).csv"
TIME_COL    = "last_tx_date"
SCORE_COL   = "fico8"
GOOD_CUTOFF = 670

SCORE0 = 600
ODDS0  = 10
PDO    = 20

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
pd.set_option("display.max_columns", 200)

def SafeOneHotEncoder():
    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)

def compute_block_sizes(n_months: int, train=0.70, val=0.15, test=0.15) -> Tuple[int,int,int]:
    t_train = max(1, int(round(n_months * train)))
    t_val   = max(1, int(round(n_months * val)))
    t_test  = n_months - t_train - t_val
    if t_test <= 0:
        remaining = n_months - t_train
        t_test = max(1, remaining - t_val)
        t_val  = n_months - t_train - t_test
    return t_train, t_val, t_test

def eval_cls(y_true, p_hat) -> Dict[str, float]:
    y_true = np.asarray(y_true).astype(int)
    p_hat  = np.asarray(p_hat).astype(float)
    roc = roc_auc_score(y_true, p_hat) if len(np.unique(y_true)) > 1 else np.nan
    pr  = average_precision_score(y_true, p_hat)
    bri = brier_score_loss(y_true, p_hat)
    rmse = float(np.sqrt(mean_squared_error(y_true, p_hat)))
    mae  = mean_absolute_error(y_true, p_hat)
    return {"ROC_AUC": roc, "PR_AUC": pr, "Brier": bri, "RMSE": rmse, "MAE": mae}

def safe_confusion(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred, labels=[0,1])
    if cm.shape == (2,2):
        tn, fp, fn, tp = cm.ravel()
    else:
        tn = cm[0,0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0
        fp = cm[0,1] if cm.shape[0] > 0 and cm.shape[1] > 1 else 0
        fn = cm[1,0] if cm.shape[0] > 1 and cm.shape[1] > 0 else 0
        tp = cm[1,1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0
    return tn, fp, fn, tp

def threshold_table(y_true, p, cuts=np.round(np.linspace(0.1,0.9,9),2)) -> pd.DataFrame:
    rows=[]
    y_true = np.asarray(y_true).astype(int)
    p = np.asarray(p).astype(float)
    for t in cuts:
        yhat = (p >= t).astype(int)
        tn, fp, fn, tp = safe_confusion(y_true, yhat)
        prec = tp / (tp + fp + 1e-12)
        rec  = tp / (tp + fn + 1e-12)
        f1   = 2 * prec * rec / (prec + rec + 1e-12)
        rows.append({"threshold": t, "TN": tn, "FP": fp, "FN": fn, "TP": tp,
                     "precision": prec, "recall": rec, "F1": f1})
    return pd.DataFrame(rows)

def ks_statistic(y_true, p_hat):
    from scipy.stats import ks_2samp
    y_true = np.asarray(y_true).astype(int)
    p_hat  = np.asarray(p_hat).astype(float)
    pos = p_hat[y_true == 1]
    neg = p_hat[y_true == 0]
    if len(pos)==0 or len(neg)==0:
        return np.nan
    return ks_2samp(pos, neg).statistic

def lift_at_k(y_true, p_hat, k=0.1):
    y_true = np.asarray(y_true).astype(int)
    p_hat  = np.asarray(p_hat).astype(float)
    n = len(y_true)
    if n == 0:
        return np.nan
    k_n = max(1, int(round(k * n)))
    order = np.argsort(-p_hat)
    y_top = y_true[order][:k_n]
    base_rate = y_true.mean() + 1e-12
    return (y_top.mean() / base_rate)

def prob_to_score(p, score0=SCORE0, odds0=ODDS0, pdo=PDO):
    p = np.clip(p, 1e-6, 1 - 1e-6)
    odds = p / (1 - p)
    factor = pdo / np.log(2.0)
    offset = score0 - factor * np.log(odds0)
    return offset + factor * np.log(odds)

def build_preprocessor(categorical_cols, numeric_cols, scaled=False):
    transformers = []
    if len(categorical_cols) > 0:
        cat_proc = Pipeline([
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", SafeOneHotEncoder()),
        ])
        transformers.append(("cat", cat_proc, categorical_cols))
    if len(numeric_cols) > 0:
        if scaled:
            num_proc = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
            ])
        else:
            num_proc = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
            ])
        transformers.append(("num", num_proc, numeric_cols))
    return ColumnTransformer(transformers) if transformers else "passthrough"

def make_expanding_folds(months, k=3, train=0.70, val=0.15, test=0.15):
    N = len(months)
    folds = []
    for i in range(1, k+1):
        size_i = max(3, (N * i) // (k + 1))
        t, v, s = compute_block_sizes(size_i, train, val, test)
        tr_end  = min(t, N - 2)
        val_end = min(tr_end + v, N - 1)
        te_end  = min(val_end + s, N)
        tr = months[:tr_end]
        va = months[tr_end:val_end]
        te = months[val_end:te_end]
        if tr and va and te:
            folds.append((tr, va, te))
    return folds

"""**IMPLEMENTING DIFFERENT BASELINES**"""

df = pd.read_csv(CSV_PATH)

if "tradelines_total" in df.columns:
    df["thin_file_flag"] = (df["tradelines_total"] < 3).astype(int)
else:
    df["thin_file_flag"] = 0

assert TIME_COL in df.columns, f"{TIME_COL} not found."
assert SCORE_COL in df.columns, f"{SCORE_COL} not found."

df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors="coerce")
df = df.dropna(subset=[TIME_COL]).copy()
df["ym"] = df[TIME_COL].dt.to_period("M").astype(str)
months = sorted(df["ym"].unique().tolist())
print(f"Months detected ({len(months)}): {months}")

df["good"] = (df[SCORE_COL] >= GOOD_CUTOFF).astype(int)

exclude_cols = {
    "customer_id", "dob", "gender", "ym", TIME_COL,
    SCORE_COL, "vantage4",
    "fraud_flag", "fraud_rate", "n_fraud_tx", "total_tx",
    "good"
}
feature_candidates = [c for c in df.columns if c not in exclude_cols]
numeric_cols     = [c for c in feature_candidates if pd.api.types.is_numeric_dtype(df[c])]
categorical_cols = [c for c in feature_candidates if pd.api.types.is_object_dtype(df[c])]

print(f"Numeric features ({len(numeric_cols)}): {numeric_cols[:15]}")
print(f"Categorical features ({len(categorical_cols)}): {categorical_cols[:15]}")

preprocess_tree   = build_preprocessor(categorical_cols, numeric_cols, scaled=False)
preprocess_scaled = build_preprocessor(categorical_cols, numeric_cols, scaled=True)

models = {
    "logistic": (preprocess_scaled, LogisticRegression(max_iter=500, class_weight="balanced", random_state=RANDOM_STATE)),
    "svm_rbf":  (preprocess_scaled, SVC(kernel="rbf", probability=True, class_weight="balanced", random_state=RANDOM_STATE)),
    "knn":      (preprocess_scaled, KNeighborsClassifier(n_neighbors=15)),
    "rf":       (preprocess_tree,   RandomForestClassifier(n_estimators=400, random_state=RANDOM_STATE, n_jobs=-1, class_weight="balanced")),
    "gbrt":     (preprocess_tree,   GradientBoostingClassifier(random_state=RANDOM_STATE)),
}

"""**TRAIN, TEST AND VALIDATION PART**"""

TRAIN_M, VAL_M, TEST_M = compute_block_sizes(len(months), 0.70, 0.15, 0.15)
window = TRAIN_M + VAL_M + TEST_M
n_folds = max(1, len(months) - window + 1)
print(f"Rolling CV config -> months={len(months)} | train={TRAIN_M} val={VAL_M} test={TEST_M} | folds={n_folds}")

expanding_folds = make_expanding_folds(months, k=3, train=0.70, val=0.15, test=0.15)
print(f"Expanding-window folds available: {len(expanding_folds)}")

use_expanding = len(expanding_folds) > 0
if use_expanding:
    fold_iter = enumerate(expanding_folds)
    print("Using expanding-window folds")
else:
    fold_iter = (
        (fold, (
            months[fold : fold + TRAIN_M],
            months[fold + TRAIN_M : fold + TRAIN_M + VAL_M],
            months[fold + TRAIN_M + VAL_M : fold + window],
        ))
        for fold in range(n_folds)
    )
    print("Using automatic rolling split")

"""**CROSS VALIDATION FOLDS**"""

rows_metrics = []

for fold, (train_months, val_months, test_months) in fold_iter:
    df_train = df[df["ym"].isin(train_months)]
    df_val   = df[df["ym"].isin(val_months)]
    df_test  = df[df["ym"].isin(test_months)]
    if df_train.empty or df_val.empty or df_test.empty:
        continue

    X_train = df_train[categorical_cols + numeric_cols]; y_train = df_train["good"].values
    X_val   = df_val[categorical_cols + numeric_cols];   y_val   = df_val["good"].values
    X_test  = df_test[categorical_cols + numeric_cols];  y_test  = df_test["good"].values

    print(f"\nFold {fold}: train={train_months} | val={val_months} | test={test_months}")

    train_months_sorted = sorted(df_train["ym"].unique().tolist())
    cal_tail = max(1, int(round(0.2 * len(train_months_sorted))))
    train_core_months = train_months_sorted[:-cal_tail] if len(train_months_sorted) > cal_tail else train_months_sorted
    train_cal_months  = train_months_sorted[-cal_tail:] if cal_tail > 0 else train_months_sorted

    df_tr_core = df_train[df_train["ym"].isin(train_core_months)]
    df_tr_cal  = df_train[df_train["ym"].isin(train_cal_months)]

    X_tr_core = df_tr_core[categorical_cols + numeric_cols]; y_tr_core = df_tr_core["good"].values
    X_tr_cal  = df_tr_cal[categorical_cols + numeric_cols];  y_tr_cal  = df_tr_cal["good"].values

    for name, (prep, clf) in models.items():
        base_pipe = Pipeline([("prep", prep), ("model", clf)])
        base_pipe.fit(X_tr_core, y_tr_core)

        cal = CalibratedClassifierCV(estimator=base_pipe, method="sigmoid", cv="prefit")
        cal.fit(X_tr_cal, y_tr_cal)

        p_val  = cal.predict_proba(X_val)[:, 1]
        p_test = cal.predict_proba(X_test)[:, 1]

        m_val  = eval_cls(y_val, p_val)
        m_test = eval_cls(y_test, p_test)

        m_val["KS"]        = ks_statistic(y_val, p_val)
        m_val["LIFT@10%"]  = lift_at_k(y_val, p_val, k=0.10)
        m_test["KS"]       = ks_statistic(y_test, p_test)
        m_test["LIFT@10%"] = lift_at_k(y_test, p_test, k=0.10)

        rows_metrics.append({"fold": fold, "model": name, "split": "val",  **m_val})
        rows_metrics.append({"fold": fold, "model": name, "split": "test", **m_test})

        print(
            f"  {name:<8} | VAL: AUC={m_val['ROC_AUC']:.3f} PR={m_val['PR_AUC']:.3f} "
            f"Brier={m_val['Brier']:.3f} KS={m_val['KS']:.3f} "
            f"| TEST: AUC={m_test['ROC_AUC']:.3f} PR={m_test['PR_AUC']:.3f} "
            f"Brier={m_test['Brier']:.3f} KS={m_test['KS']:.3f}"
        )

metrics_df = pd.DataFrame(rows_metrics)

summary_df = (metrics_df
    .groupby(["model","split"], as_index=False)[["ROC_AUC","PR_AUC","Brier","RMSE","MAE"]]
    .mean(numeric_only=True)
    .sort_values(by=["split","ROC_AUC"], ascending=[True, False]))

print("\nAveraged metrics by model & split (CV): ")
print(summary_df.round(4).to_string(index=False))

by_ms = (metrics_df
    .groupby(["model","split"], as_index=False)
    .agg(ROC_mean=("ROC_AUC","mean"),
         ROC_std =("ROC_AUC", lambda x: np.nan if len(x)<2 else np.std(x, ddof=1)),
         PR_mean =("PR_AUC","mean"),
         Brier_mean=("Brier","mean")))

gaps = by_ms.pivot(index="model", columns="split", values="ROC_mean").reset_index()
gaps = gaps.rename_axis(None, axis=1).fillna(np.nan)
gaps["gen_gap_auc"] = gaps.get("val", np.nan) - gaps.get("test", np.nan)

instab = by_ms[by_ms["split"]=="test"][["model","ROC_std"]].rename(columns={"ROC_std":"test_auc_std"})
diag = gaps.merge(instab, on="model", how="left").sort_values("gen_gap_auc", ascending=False)

print("\nGeneralization diagnostics: ")
print(diag.round(4).to_string(index=False))

multi_fold = metrics_df["fold"].nunique() >= 2
if multi_fold:
    diag["overfit_flag"]   = (diag["gen_gap_auc"] > 0.05) | (diag["test_auc_std"] > 0.04)
    diag["underfit_flag"]  = (diag["val"] < 0.65) & (diag["test"] < 0.62) & (diag["gen_gap_auc"].abs() < 0.02)
    print("\nHeuristic flags: ")
    print(diag[["model","val","test","gen_gap_auc","test_auc_std","overfit_flag","underfit_flag"]]
          .round(4).to_string(index=False))
else:
    print("\nNote: Only one fold detected; variance-based flags are suppressed for reliability.")

"""**FINAL MODEL CHOSEN BASED ON FAVORABLE METRIC EVALUATION**"""

val_tbl = summary_df[summary_df["split"]=="val"].copy()
val_tbl = val_tbl.sort_values(by=["ROC_AUC","Brier"], ascending=[False, True]).reset_index(drop=True)
winner = val_tbl.iloc[0]["model"]
print(f"\nWinner (by VAL ROC_AUC): {winner}")
print(val_tbl.round(4).to_string(index=False))

TRAIN_M, VAL_M, TEST_M = compute_block_sizes(len(months), 0.70, 0.15, 0.15)
final_test_months = months[-TEST_M:]
train_months_all  = months[:-TEST_M]

cal_tail          = max(1, int(round(0.2 * len(train_months_all))))
train_core_months = train_months_all[:-cal_tail] if len(train_months_all) > cal_tail else train_months_all
train_cal_months  = train_months_all[-cal_tail:] if cal_tail > 0 else train_months_all

df_train_core = df[df["ym"].isin(train_core_months)]
df_train_cal  = df[df["ym"].isin(train_cal_months)]
df_test_final = df[df["ym"].isin(final_test_months)]

X_tr_core = df_train_core[categorical_cols + numeric_cols]; y_tr_core = df_train_core["good"].values
X_tr_cal  = df_train_cal[categorical_cols + numeric_cols];  y_tr_cal  = df_train_cal["good"].values
X_te      = df_test_final[categorical_cols + numeric_cols]; y_te      = df_test_final["good"].values

prep_w, clf_w = models[winner]

final_base = Pipeline([("prep", prep_w), ("model", clf_w)]).fit(X_tr_core, y_tr_core)
final_cal  = CalibratedClassifierCV(estimator=final_base, method="sigmoid", cv="prefit")
final_cal.fit(X_tr_cal, y_tr_cal)

p_final = final_cal.predict_proba(X_te)[:, 1]

final_metrics = {
    "ROC_AUC": roc_auc_score(y_te, p_final) if len(np.unique(y_te))>1 else np.nan,
    "PR_AUC":  average_precision_score(y_te, p_final),
    "Brier":   brier_score_loss(y_te, p_final),
    "RMSE":    float(np.sqrt(mean_squared_error(y_te, p_final))),
    "MAE":     mean_absolute_error(y_te, p_final),
    "KS":      ks_statistic(y_te, p_final),
    "LIFT@10%": lift_at_k(y_te, p_final, k=0.10),
}
print("\nFinal Holdout (latest months) — Calibrated P(creditworthy):")
for k,v in final_metrics.items():
    print(f"{k:>8s}: {v:.4f}")

tt = threshold_table(y_te, p_final)
print("\nThreshold table (precision/recall/F1) on P(good):")
print(tt.round(4).to_string(index=False))

scores_final = prob_to_score(p_final, score0=SCORE0, odds0=ODDS0, pdo=PDO)

cols = ["ym"]
if "customer_id" in df_test_final.columns:
    cols = ["customer_id"] + cols
preview = df_test_final[cols].copy()
preview["y_true_good"]    = y_te
preview["p_creditworthy"] = p_final
preview["credit_score"]   = scores_final

print("\nPreview: final holdout predictions (first 30 rows):")
with pd.option_context("display.max_columns", None):
    print(preview.head(30).to_string(index=False))

feat_rows = []
for c in categorical_cols:
    feat_rows.append({"feature": c, "dtype": "categorical", "pandas_dtype": str(df[c].dtype)})
for c in numeric_cols:
    feat_rows.append({"feature": c, "dtype": "numeric", "pandas_dtype": str(df[c].dtype)})
features_df = pd.DataFrame(feat_rows).sort_values(by=["dtype","feature"]).reset_index(drop=True)

print("\nFeatures used (first 40 rows):")
print(features_df.head(40).to_string(index=False))
print(f"\nTotal are numeric: {len(numeric_cols)}, categorical: {len(categorical_cols)} "
      f"which totals {len(numeric_cols)+len(categorical_cols)}")

"""**AGGREGATED FEATURE CALCULATION**"""

import numpy as np
import pandas as pd

def _has(*cols):
    return all(c in df.columns for c in cols)

def zdiv(a, b):
    a = np.asarray(a, dtype="float64")
    b = np.asarray(b, dtype="float64")
    out = np.divide(a, b, out=np.zeros_like(a, dtype="float64"), where=(b!=0))
    return out

util_base = None
if "utilization_ratio" in df.columns:
    util_base = df["utilization_ratio"].astype(float)
elif "revolving_utilization" in df.columns:
    util_base = df["revolving_utilization"].astype(float)

income = df["monthly_salary"].astype(float) if "monthly_salary" in df.columns else pd.Series(np.nan, index=df.index)

if "thin_file_flag" not in df.columns and "tradelines_total" in df.columns:
    df["thin_file_flag"] = (df["tradelines_total"] < 3).astype(int)

if _has("tradelines_open","tradelines_total"):
    df["open_ratio"] = zdiv(df["tradelines_open"], df["tradelines_total"])
else:
    df["open_ratio"] = np.nan

if _has("revolving_limit_sum","tradelines_open"):
    df["rev_limit_per_line"] = zdiv(df["revolving_limit_sum"], df["tradelines_open"])
else:
    df["rev_limit_per_line"] = np.nan

if _has("revolving_balance_sum","total_balance_sum"):
    df["rev_share_total_bal"] = zdiv(df["revolving_balance_sum"], df["total_balance_sum"])
else:
    df["rev_share_total_bal"] = np.nan

if util_base is not None:
    df["util_clip"] = util_base.clip(lower=0, upper=1.5)
else:
    df["util_clip"] = np.nan

if "monthly_salary" in df.columns:
    inc = income.replace(0, np.nan)
    if "total_balance_sum" in df.columns:
        df["balance_to_income"] = (df["total_balance_sum"].astype(float) / inc).fillna(0)
    else:
        df["balance_to_income"] = np.nan
    if "revolving_balance_sum" in df.columns:
        df["revolving_to_income"] = (df["revolving_balance_sum"].astype(float) / inc).fillna(0)
    else:
        df["revolving_to_income"] = np.nan
    if "revolving_limit_sum" in df.columns:
        df["limit_to_income"] = (df["revolving_limit_sum"].astype(float) / inc).fillna(0)
    else:
        df["limit_to_income"] = np.nan
else:
    df["balance_to_income"]   = np.nan
    df["revolving_to_income"] = np.nan
    df["limit_to_income"]     = np.nan

if "delinq_24mo_count" in df.columns:
    df["any_delinq_24m"] = (df["delinq_24mo_count"].astype(float) > 0).astype(int)
else:
    df["any_delinq_24m"] = np.nan
if "worst_delinq_24mo" in df.columns:
    df["worst_delinq_1plus"] = (df["worst_delinq_24mo"].astype(float) >= 1).astype(int)
else:
    df["worst_delinq_1plus"] = np.nan

if "inquiries_12mo_hard" in df.columns:
    df["hard_inq_3plus"] = (df["inquiries_12mo_hard"].astype(float) >= 3).astype(int)
else:
    df["hard_inq_3plus"] = np.nan

if _has("inquiries_12mo_hard","tradelines_open"):
    df["inq_per_open_line"] = zdiv(df["inquiries_12mo_hard"], df["tradelines_open"])
else:
    df["inq_per_open_line"] = np.nan

def bucketize(x, bins, labels):
    return pd.cut(x, bins=bins, labels=labels, include_lowest=True, right=False)

if "avg_account_age_months" in df.columns:
    df["seasoning_bucket"] = bucketize(
        df["avg_account_age_months"].astype(float).fillna(0),
        bins=[0, 12, 36, 84, 10_000],
        labels=["<12m","12–36m","36–84m","≥84m"]
    )
else:
    df["seasoning_bucket"] = pd.Series(pd.Categorical([np.nan]*len(df), categories=["<12m","12–36m","36–84m","≥84m"]))

if "age" in df.columns:
    df["age_band"] = bucketize(
        df["age"].astype(float).fillna(0),
        bins=[0, 25, 35, 50, 200],
        labels=["<25","25–34","35–49","50+"]
    )
else:
    df["age_band"] = pd.Series(pd.Categorical([np.nan]*len(df), categories=["<25","25–34","35–49","50+"]))

if "experience_years" in df.columns:
    df["experienced_worker"] = (df["experience_years"].astype(float) >= 3).astype(int)
else:
    df["experienced_worker"] = np.nan

if "last_tx_date" in df.columns:
    ref_date = pd.to_datetime(df.get("score_date", df["last_tx_date"].max()))
    df["recency_days"] = (pd.to_datetime(ref_date) - pd.to_datetime(df["last_tx_date"])).dt.days.clip(lower=0)
else:
    df["recency_days"] = np.nan

df["tx_frequency"] = df["n_daily_tx"] if "n_daily_tx" in df.columns else np.nan
df["avg_ticket"]   = df["avg_tx_amount"] if "avg_tx_amount" in df.columns else np.nan

if "monthly_salary" in df.columns:
    inc = income.replace(0, np.nan)
    if "total_spend" in df.columns:
        df["spend_to_income"] = (df["total_spend"].astype(float) / inc).fillna(0)
    else:
        df["spend_to_income"] = np.nan
    if "avg_daily_spend" in df.columns:
        df["daily_spend_to_income"] = (df["avg_daily_spend"].astype(float) / (inc/30)).fillna(0)
    else:
        df["daily_spend_to_income"] = np.nan
else:
    df["spend_to_income"]       = np.nan
    df["daily_spend_to_income"] = np.nan

df["util_x_thin"]     = df["util_clip"] * df.get("thin_file_flag", 0)
df["inq_x_thin"]      = df.get("inquiries_12mo_hard", 0) * df.get("thin_file_flag", 0)
df["seasoned_x_util"] = df["util_clip"] * (df.get("avg_account_age_months", 0).astype(float) >= 36).astype(int)
df["income_x_util"]   = df["util_clip"] * df.get("monthly_salary", 0).astype(float)

for c in ["revolving_limit_sum","revolving_balance_sum","total_balance_sum",
          "total_tx_amount","total_spend","monthly_salary"]:
    df[f"log1p_{c}"] = np.log1p(df[c].clip(lower=0)) if c in df.columns else np.nan

exclude_cols = {
    "customer_id", "dob", "gender", "ym", "last_tx_date",
    "fico8", "vantage4", "fraud_flag", "fraud_rate", "n_fraud_tx",
    "total_tx", "good", "good_v2",
}
feature_candidates = [c for c in df.columns if c not in exclude_cols]
numeric_cols = [c for c in feature_candidates if pd.api.types.is_numeric_dtype(df[c])]
categorical_cols = [c for c in feature_candidates if pd.api.types.is_object_dtype(df[c])]

print(f"[Derived] Added engineered features. Now {len(numeric_cols)} numeric, {len(categorical_cols)} categorical.")

"""**RUNTIME FEATURES**"""

import numpy as np
import pandas as pd

def zdiv(a, b):
    a = np.asarray(a, dtype="float64")
    b = np.asarray(b, dtype="float64")
    return np.divide(a, b, out=np.zeros_like(a, dtype="float64"), where=(b!=0))

def compute_runtime_features(
    tx_df: pd.DataFrame,
    now: pd.Timestamp | None = None,
    windows = ("15min","1H","6H","24H"),
    baseline_window = "90D"
) -> pd.DataFrame:
    if now is None:
        now = pd.Timestamp.utcnow()

    base_cols = ["customer_id","minutes_since_last_tx","hour_of_day","day_of_week","is_weekend",
                 "last_amount","pending_auth_sum","zscore_last_amount_90d",
                 "ratio_last_amount_to_p50_90d","burst_flag","consecutive_declines"]
    for w in windows:
        base_cols += [f"tx_count_{w}", f"tx_sum_{w}", f"tx_avg_{w}",
                      f"unique_merchants_{w}", f"unique_mcc_{w}",
                      f"decline_rate_{w}", f"card_present_ratio_{w}"]
    if tx_df is None or tx_df.empty:
        return pd.DataFrame(columns=base_cols)

    d = tx_df.copy()
    d = d.dropna(subset=["customer_id","ts"])
    d["ts"] = pd.to_datetime(d["ts"], utc=True)
    d = d.sort_values(["customer_id","ts"])
    d["amount"] = pd.to_numeric(d.get("amount", 0), errors="coerce").fillna(0.0)

    status = d.get("status")
    if status is not None:
        status = status.astype(str).str.lower()
        d["is_declined"] = status.eq("declined")
        d["is_authorized_open"] = status.eq("authorized")
    else:
        d["is_declined"] = False
        d["is_authorized_open"] = False

    if "entry_mode" in d.columns:
        entry = d["entry_mode"].astype(str).str.lower()
        d["is_card_present"] = entry.isin(["chip","swipe","tap","contact","card_present"])
    else:
        d["is_card_present"] = d.get("channel","").astype(str).str.lower().isin(["pos","in_store","card_present"])

    d["asof_now"] = now

    last_tx = d.groupby("customer_id")["ts"].max().to_frame("last_tx_ts")
    last_amt = d.groupby("customer_id")["amount"].last().to_frame("last_amount")
    feats = last_tx.join(last_amt, how="outer").reset_index()
    feats["minutes_since_last_tx"] = ((now - feats["last_tx_ts"]) / pd.Timedelta(minutes=1)).clip(lower=0)
    feats["hour_of_day"] = int(pd.Timestamp(now).tz_convert("UTC").hour) if getattr(now, "tzinfo", None) else int(pd.Timestamp(now).hour)
    feats["day_of_week"] = int(pd.Timestamp(now).dayofweek)  # 0=Mon
    feats["is_weekend"] = (feats["day_of_week"] >= 5).astype(int)

    d = d.set_index("ts")
    for w in windows:
        wmask = (d["asof_now"].iloc[0] - d.index <= pd.Timedelta(w))
        dw = d.loc[wmask]
        if dw.empty:
            continue
        g = dw.groupby("customer_id")
        chunk = pd.concat([
            g["amount"].size().rename(f"tx_count_{w}"),
            g["amount"].sum().rename(f"tx_sum_{w}"),
            g["amount"].mean().fillna(0.0).rename(f"tx_avg_{w}"),
            (g["merchant_id"].nunique().rename(f"unique_merchants_{w}") if "merchant_id" in dw.columns else pd.Series(dtype=float, name=f"unique_merchants_{w}")),
            (g["mcc"].nunique().rename(f"unique_mcc_{w}") if "mcc" in dw.columns else pd.Series(dtype=float, name=f"unique_mcc_{w}")),
            (g["is_declined"].mean().fillna(0.0).rename(f"decline_rate_{w}") if "is_declined" in dw.columns else pd.Series(dtype=float, name=f"decline_rate_{w}")),
            (g["is_card_present"].mean().fillna(0.0).rename(f"card_present_ratio_{w}") if "is_card_present" in dw.columns else pd.Series(dtype=float, name=f"card_present_ratio_{w}")),
        ], axis=1)
        feats = feats.merge(chunk, on="customer_id", how="left")

    pending = d[d["is_authorized_open"]].groupby("customer_id")["amount"].sum().rename("pending_auth_sum")
    feats = feats.merge(pending, on="customer_id", how="left")

    mask90 = (d["asof_now"].iloc[0] - d.index <= pd.Timedelta(baseline_window))
    d90 = d.loc[mask90]
    if not d90.empty:
        g90 = d90.groupby("customer_id")["amount"]
        stats = pd.concat([
            g90.mean().rename("amt_mu_90d"),
            g90.std(ddof=0).replace(0, np.nan).rename("amt_sd_90d"),
            g90.quantile(0.50).replace(0, np.nan).rename("amt_p50_90d"),
        ], axis=1).reset_index()
        feats = feats.merge(stats, on="customer_id", how="left")
        feats["zscore_last_amount_90d"] = (feats["last_amount"] - feats["amt_mu_90d"]) / feats["amt_sd_90d"]
        feats["ratio_last_amount_to_p50_90d"] = feats["last_amount"] / feats["amt_p50_90d"]
        feats = feats.drop(columns=["amt_mu_90d","amt_sd_90d","amt_p50_90d"], errors="ignore")

    mask5 = (d["asof_now"].iloc[0] - d.index <= pd.Timedelta("5min"))
    cnt5 = d.loc[mask5].groupby("customer_id")["amount"].size().rename("tx_count_5min")
    feats = feats.merge(cnt5, on="customer_id", how="left")
    feats["burst_flag"] = (feats["tx_count_5min"].fillna(0) >= 3).astype(int)
    feats = feats.drop(columns=["tx_count_5min"], errors="ignore")

    def last_k_declines(grp, k=2, lookback=10):
        tail = grp.tail(lookback)
        if "is_declined" not in tail.columns or tail.empty:
            return 0
        return int(tail["is_declined"].tail(k).all())

    cons_dec = (d.reset_index()
                  .sort_values(["customer_id","ts"])
                  .groupby("customer_id")
                  .apply(lambda g: last_k_declines(g, k=2, lookback=10))
                  .rename("consecutive_declines")
                  .reset_index())
    feats = feats.merge(cons_dec, on="customer_id", how="left")

    for c in feats.columns:
        if c != "customer_id":
            feats[c] = pd.to_numeric(feats[c], errors="ignore")
    numeric = feats.columns.drop("customer_id")
    feats[numeric] = feats[numeric].fillna(0)

    return feats

def integrate_offline_and_runtime(df_offline: pd.DataFrame, runtime_feats: pd.DataFrame) -> pd.DataFrame:
    out = df_offline.merge(runtime_feats, on="customer_id", how="left", suffixes=("", "_rt"))
    if set(["revolving_limit_sum","revolving_balance_sum","pending_auth_sum"]).issubset(out.columns):
        out["rt_utilization"] = zdiv(
            out["revolving_balance_sum"].astype(float) + out["pending_auth_sum"].astype(float),
            out["revolving_limit_sum"].astype(float)
        )
        out["rt_util_clip"] = out["rt_utilization"].clip(lower=0, upper=1.5)
    else:
        out["rt_utilization"] = np.nan
        out["rt_util_clip"] = np.nan

    if "thin_file_flag" in out.columns and "minutes_since_last_tx" in out.columns:
        out["thin_x_inactivity"] = out["thin_file_flag"].astype(float) * out["minutes_since_last_tx"].astype(float)

    return out

if "tx_df" not in globals():
    now = pd.Timestamp.utcnow()
    rows = []
    for _, r in df.iterrows():
        cid = r.get("customer_id")
        if pd.isna(cid):
            continue
        n_today = int(min(max((r.get("n_daily_tx") or 0), 0), 3))  # 0..3
        amt = r.get("avg_tx_amount")
        if pd.isna(amt) or (amt is None) or (amt == 0):
            denom = max(int(r.get("n_daily_tx") or 1), 1)
            proxy = (r.get("avg_daily_spend") or 0) / denom
            amt = proxy if proxy and proxy > 0 else np.random.uniform(15, 65)
        amt = float(amt)

        for i in range(n_today):
            rows.append({
                "customer_id": cid,
                "ts": now - pd.Timedelta(minutes=10*i + np.random.randint(0,4)),
                "amount": amt,
                "status": "cleared",
                "mcc": 5411 if i % 2 == 0 else 5999,
                "merchant_id": f"M{(hash(str(cid))+i) % 2000:04d}",
                "channel": "pos" if i % 2 == 0 else "ecommerce",
                "entry_mode": "chip" if i % 2 == 0 else "manual",
            })
    tx_df = pd.DataFrame(rows, columns=["customer_id","ts","amount","status","mcc","merchant_id","channel","entry_mode"])

_now = pd.Timestamp.utcnow()
rt_feats = compute_runtime_features(
    tx_df,
    now=_now,
    windows=("15min","1H","6H","24H"),
    baseline_window="90D"
)
df_scoring = integrate_offline_and_runtime(df, rt_feats)

print(f"[Scoring] Final features: {df_scoring.shape[1]} columns "
      f"({df.shape[1]} offline + {rt_feats.shape[1]-1} runtime incl. interactions).")

df.head()

df_scoring

print("df shape:", df.shape)
print("tx_df shape:", tx_df.shape if 'tx_df' in globals() else None)
print("rt_feats shape:", rt_feats.shape)
print("df_scoring shape:", df_scoring.shape)

nulls = df_scoring.isna().mean().sort_values(ascending=False).head(15)
print("[Top null columns]\n", nulls)

display(df_scoring.head(3))

def _num(x): return pd.to_numeric(x, errors="coerce")
def _entropy_from_counts(s):
    total = float(s.sum())
    if total <= 0: return 0.0
    p = (s / total).values
    p = p[p>0]
    return float(-(p*np.log(p)).sum())

def add_runtime_mcc_features(df_scoring, tx_df):
    if tx_df is None or tx_df.empty or "mcc" not in tx_df.columns:
        df_scoring["cf_expense_entropy_30d"] = np.nan
        df_scoring["beh_cash_intensity_30d"] = np.nan
        return df_scoring
    d = tx_df.copy()
    d["ts"] = pd.to_datetime(d["ts"], utc=True)
    d["amount"] = _num(d["amount"]).fillna(0.0)
    d = d[d["ts"] >= (pd.Timestamp.utcnow() - pd.Timedelta("30D"))]
    ent = (d.groupby(["customer_id","mcc"])["amount"].sum()
             .reset_index()
             .groupby("customer_id")
             .apply(lambda g: _entropy_from_counts(g.set_index("mcc")["amount"]))
             .rename("cf_expense_entropy_30d")
             .reset_index())
    total = d.groupby("customer_id")["amount"].sum()
    cash = d[d["mcc"].astype(str)=="6011"].groupby("customer_id")["amount"].sum()
    cash_share = (cash/total).replace([np.inf,-np.inf], np.nan).fillna(0.0).rename("beh_cash_intensity_30d").reset_index()
    out = df_scoring.merge(ent, on="customer_id", how="left")
    out = out.merge(cash_share, on="customer_id", how="left")
    return out

df_scoring = add_runtime_mcc_features(df_scoring, tx_df)

def add_seasonality(df):
    if "ym" not in df.columns:
        df["beh_seasonality_index_12m"] = np.nan
        df["beh_holiday_spike_flag"] = np.nan
        return df
    w = df.copy()
    w["ym"] = pd.to_datetime(w["ym"], errors="coerce").dt.to_period("M").dt.to_timestamp()
    w["spend_proxy"] = _num(w.get("total_spend", np.nan)).fillna(_num(w.get("avg_daily_spend", np.nan))*30)
    w = w.sort_values(["customer_id","ym"])
    g = w.groupby("customer_id", group_keys=False)
    def compute(gr):
        s = gr["spend_proxy"].astype(float)
        tail = s.tail(12).dropna()
        idx = float(tail.std(ddof=0)/tail.mean()) if len(tail)>=4 and tail.mean()>0 else np.nan
        med3 = s.shift(1).rolling(3, min_periods=2).median()
        spike = int((s > 1.5*med3).tail(1).fillna(False).iloc[0]) if len(s) else 0
        gr.loc[gr.index.max(),"beh_seasonality_index_12m"] = idx
        gr.loc[gr.index.max(),"beh_holiday_spike_flag"] = spike
        return gr
    w = g.apply(compute)
    w[["beh_seasonality_index_12m","beh_holiday_spike_flag"]] = (
        w.groupby("customer_id")[["beh_seasonality_index_12m","beh_holiday_spike_flag"]].ffill()
    )
    latest = w.sort_values(["customer_id","ym"]).groupby("customer_id").tail(1)
    return df_scoring.merge(latest[["customer_id","beh_seasonality_index_12m","beh_holiday_spike_flag"]],
                            on="customer_id", how="left")

df_scoring = add_seasonality(df if "ym" in df.columns else df_scoring)

import numpy as np
import pandas as pd

def add_geo_macro(df_scoring, macro_path="/content/macrodata_filled_realistic.csv"):
    out = df_scoring.copy()

    if "primary_state" not in out.columns:
        for c in ["bf_local_unemployment","bf_local_inflation","bf_regional_perf_index"]:
            if c not in out.columns:
                out[c] = np.nan
        return out

    out["primary_state"] = out["primary_state"].astype(str).str.strip()

    if "ym" in out.columns:
        out["ym"] = pd.to_datetime(out["ym"], errors="coerce").dt.to_period("M").dt.to_timestamp()

    try:
        macro = pd.read_csv(macro_path)
    except Exception:
        for c in ["bf_local_unemployment","bf_local_inflation","bf_regional_perf_index"]:
            if c not in out.columns:
                out[c] = np.nan
        return out

    macro = macro.rename(columns={c: c.strip().lower() for c in macro.columns})

    geo_candidates  = ["primary_state","state","state_code","state_abbrev","region","province"]
    time_candidates = ["ym","date","month","asof_date"]

    macro_geo  = next((c for c in geo_candidates  if c in macro.columns), None)
    macro_time = next((c for c in time_candidates if c in macro.columns), None)

    if macro_geo is None:
        for c in ["bf_local_unemployment","bf_local_inflation","bf_regional_perf_index"]:
            if c not in out.columns:
                out[c] = np.nan
        return out

    macro[macro_geo] = macro[macro_geo].astype(str).str.strip()

    if macro_time is not None:
        macro[macro_time] = pd.to_datetime(macro[macro_time], errors="coerce").dt.to_period("M").dt.to_timestamp()

    rename_map = {}
    for cand in ["unemployment_rate","unemployment","unemp_rate","jobless_rate"]:
        if cand in macro.columns:
            rename_map[cand] = "bf_local_unemployment"; break
    for cand in ["inflation","cpi_yoy","cpi","inflation_rate"]:
        if cand in macro.columns:
            rename_map[cand] = "bf_local_inflation"; break
    for cand in ["regional_perf_index","regional_index","stress_index","lending_index"]:
        if cand in macro.columns:
            rename_map[cand] = "bf_regional_perf_index"; break

    macro = macro.rename(columns=rename_map)

    keep_cols = [macro_geo] + [c for c in ["bf_local_unemployment","bf_local_inflation","bf_regional_perf_index"] if c in macro.columns]
    if macro_time is not None:
        keep_cols.append(macro_time)
    macro = macro[keep_cols].copy()

    if macro_geo != "primary_state":
        macro = macro.rename(columns={macro_geo: "primary_state"})

    out["primary_state"]   = out["primary_state"].str.upper()
    macro["primary_state"] = macro["primary_state"].str.upper()

    left_on  = ["primary_state"]
    right_on = ["primary_state"]
    if ("ym" in out.columns) and (macro_time is not None):
        left_on.append("ym")
        right_on.append(macro_time)

    merged = out.merge(macro, how="left", left_on=left_on, right_on=right_on)

    if ("ym" in out.columns) and (macro_time is not None) and (macro_time != "ym") and (macro_time in merged.columns):
        merged = merged.drop(columns=[macro_time])

    for c in ["bf_local_unemployment","bf_local_inflation","bf_regional_perf_index"]:
        if c not in merged.columns:
            merged[c] = np.nan

    return merged

m = pd.read_csv("/content/macrodata_filled_realistic.csv")
print("macro columns:", list(m.columns))
print(m.head(3))

import numpy as np
import pandas as pd

def _pct_change(series, periods=12):
    s = pd.to_numeric(series, errors="coerce")
    return s.pct_change(periods=periods)

def _roll_std(series, window):
    s = pd.to_numeric(series, errors="coerce")
    return s.rolling(window, min_periods=max(3, window//3)).std()

def build_national_macro_features(csv_path="/content/macrodata_filled_realistic.csv"):
    m = pd.read_csv(csv_path)
    date_col = "Unnamed: 0"
    if date_col not in m.columns:
        date_col = m.columns[0]
    m[date_col] = pd.to_datetime(m[date_col], errors="coerce")
    m = m.rename(columns={date_col: "date"})
    m = m.sort_values("date").dropna(subset=["date"])

    UNRATE   = pd.to_numeric(m.get("UNRATE"), errors="coerce")
    CPI      = pd.to_numeric(m.get("CPIAUCSL"), errors="coerce")
    INDPRO   = pd.to_numeric(m.get("INDPRO"), errors="coerce")
    RPI      = pd.to_numeric(m.get("RPI"), errors="coerce")
    GS10     = pd.to_numeric(m.get("GS10"), errors="coerce")
    GS1      = pd.to_numeric(m.get("GS1"), errors="coerce")
    FEDFUNDS = pd.to_numeric(m.get("FEDFUNDS"), errors="coerce")
    SP500    = pd.to_numeric(m.get("SP500"), errors="coerce")
    NASDAQ   = pd.to_numeric(m.get("NASDAQ"), errors="coerce")
    AAA      = pd.to_numeric(m.get("AAA"), errors="coerce")
    REGIME   = m.get("Regime")

    out = pd.DataFrame({"ym": m["date"].dt.to_period("M").dt.to_timestamp()})

    out["bf_unemployment_rate"] = UNRATE
    out["bf_unemp_chg_3m"]      = UNRATE.diff(3)

    out["bf_inflation_yoy"]     = _pct_change(CPI, periods=12)

    out["bf_indpro_yoy"]        = _pct_change(INDPRO, periods=12)
    out["bf_rpi_yoy"]           = _pct_change(RPI, periods=12)

    out["bf_term_spread_10y_1y"] = (GS10 - GS1)
    out["bf_real_fedfunds"]      = FEDFUNDS - (out["bf_inflation_yoy"]*100.0)

    sp_ret = _pct_change(SP500, periods=3)
    out["bf_sp500_ret_3m"]  = sp_ret
    out["bf_sp500_vol_6m"]  = _roll_std(_pct_change(SP500, periods=1), window=6)

    if AAA.notna().any() and GS10.notna().any():
        out["bf_credit_spread_aaa_10y"] = (AAA - GS10)
    else:
        out["bf_credit_spread_aaa_10y"] = np.nan

    if REGIME is not None:
        try:
            out["bf_macro_regime"] = pd.to_numeric(REGIME, errors="coerce")
        except Exception:
            out["bf_macro_regime"] = REGIME.astype(str)

    return out

def merge_macro_national(df_scoring, macro_csv="/content/macrodata_filled_realistic.csv"):
    macro = build_national_macro_features(macro_csv)
    out = df_scoring.copy()

    if "ym" in out.columns:
        out["ym"] = pd.to_datetime(out["ym"], errors="coerce").dt.to_period("M").dt.to_timestamp()
        out = out.merge(macro, on="ym", how="left")
    else:
        latest = macro.sort_values("ym").tail(1)
        for c in [c for c in macro.columns if c != "ym"]:
            out[c] = latest[c].values[0]
    return out


df_scoring = merge_macro_national(df_scoring, "/content/macrodata_filled_realistic.csv")

"""**MADE DATASET WITH ALL THE MACRO DATA FEATURES**"""

df_scoring

"""**WE NEED DATA WITH RUNTIME FEATURES - SO CLASSIFYING DATA WITH THOSE ITSELF**"""

KEYS   = ["customer_id", "last_tx_ts", "ym"]
LABELS = ["fraud_flag", "good"]

RUNTIME_PREFIXES = (
    "tx_count_", "tx_sum_", "tx_avg_",
    "unique_merchants_", "unique_mcc_",
    "decline_rate_", "card_present_ratio_",
    "cf_", "beh_", "rt_"
)

RUNTIME_EXACT = [
    "minutes_since_last_tx", "hour_of_day", "day_of_week", "is_weekend",
    "recency_days", "tx_frequency", "avg_ticket",
    "last_amount", "pending_auth_sum", "zscore_last_amount_90d", "ratio_last_amount_to_p50_90d",
    "burst_flag", "consecutive_declines", "thin_x_inactivity",
]

cols = df_scoring.columns.tolist()

runtime_by_prefix = [c for c in cols if any(c.startswith(p) for p in RUNTIME_PREFIXES)]
runtime_by_exact  = [c for c in RUNTIME_EXACT if c in cols]

keep_cols = []
keep_cols += [c for c in KEYS   if c in cols]
keep_cols += [c for c in LABELS if c in cols]
keep_cols += runtime_by_prefix
keep_cols += runtime_by_exact

seen = set()
keep_cols = [c for c in keep_cols if not (c in seen or seen.add(c))]

df_runtime_only = df_scoring[keep_cols].copy()

OFFLINE_BLOCKLIST_PREFIXES = (
    "fico", "vantage", "tradelines_", "revolving_limit_sum", "revolving_balance_sum",
    "revolving_utilization", "total_balance_sum", "delinq_", "worst_delinq",
    "inquiries_", "has_collection", "collection_balance_total", "has_bankruptcy",
    "months_since_oldest_account", "avg_account_age_months", "balance_to_income",
    "revolving_to_income", "limit_to_income", "open_ratio", "rev_limit_per_line",
    "rev_share_total_bal", "util_clip", "any_delinq_24m", "hard_inq_3plus",
    "inq_per_open_line", "seasoning_bucket", "age_band", "experienced_worker",
    "utilization_ratio", "log1p_", "bf_"  # macros
)
accidental_offline = [c for c in df_runtime_only.columns if c.startswith(OFFLINE_BLOCKLIST_PREFIXES)]
if accidental_offline:
    print("[WARN] These look like offline/static columns but slipped in:", accidental_offline)

dropped = [c for c in cols if c not in df_runtime_only.columns]
print(f"[Result] Kept {len(df_runtime_only.columns)} columns, Dropped {len(dropped)} columns.")
print("Sample kept columns:", df_runtime_only.columns.tolist())

df_runtime_only.shape

df_runtime_only

df_runtime_only['last_tx_ts'] = pd.to_datetime(df_runtime_only['last_tx_ts'], unit='ns', utc=True)
df_runtime_only['last_tx_ts_local'] = df_runtime_only['last_tx_ts'].dt.tz_convert('America/New_York')

df_runtime_only

df_runtime_only.to_csv("df_scoring_runtime_only.csv", index=False)

import json
import re
import numpy as np
import pandas as pd
from pathlib import Path

SCHEMA_PATH = Path("/content/runtime_feature_registry.json")
SAVE_LOCKED_AS = Path("/content/df_runtime_only_locked.csv")

DROP_EXTRAS = False


def _infer_default(col: str) -> float | int:
    if re.search(r"(flag|is_weekend|consecutive_declines)$", col):
        return 0
    if re.search(r"(tx_count|unique_merchants|unique_mcc)$", col):
        return 0
    if re.search(r"(tx_sum|tx_avg|ratio_|rate_|entropy|util_|card_present_ratio|cash_intensity|seasonality)", col):
        return 0.0
    if re.search(r"(minutes_since_last_tx|recency_days)$", col):
        return 0.0
    if re.search(r"(last_amount|pending_auth_sum)$", col):
        return 0.0
    if col in {"hour_of_day","day_of_week"}:
        return 0
    if col in {"customer_id","ym","fraud_flag","good","last_tx_ts","last_tx_ts_local"}:
        return np.nan
    return 0.0

def _infer_dtype(col: str):
    if col in {"customer_id"}:
        return "object"
    if col in {"fraud_flag","good","is_weekend","burst_flag","consecutive_declines"}:
        return "int64"
    if col in {"hour_of_day","day_of_week"}:
        return "int64"
    if col in {"ym","last_tx_ts","last_tx_ts_local"}:
        return "datetime64[ns]"
    return "float64"

def save_schema(columns: list[str], path: Path):
    with path.open("w") as f:
        json.dump({"columns": columns}, f, indent=2)

def load_schema(path: Path) -> list[str] | None:
    if path.exists():
        with path.open() as f:
            return json.load(f).get("columns")
    return None

def enforce_schema(df: pd.DataFrame, expected_cols: list[str], drop_extras: bool = False) -> pd.DataFrame:
    out = df.copy()

    missing = [c for c in expected_cols if c not in out.columns]
    for c in missing:
        out[c] = _infer_default(c)

    if drop_extras:
        out = out[expected_cols]
    else:
        extra = [c for c in out.columns if c not in expected_cols]
        out = out[expected_cols + extra]

    for c in expected_cols:
        target = _infer_dtype(c)
        try:
            if target == "datetime64[ns]":
                out[c] = pd.to_datetime(out[c], errors="coerce")
            elif target == "int64":
                out[c] = pd.to_numeric(out[c], errors="coerce").fillna(0).astype("int64")
            elif target == "float64":
                out[c] = pd.to_numeric(out[c], errors="coerce")
            elif target == "object":
                out[c] = out[c].astype("string").astype("object")
        except Exception:
            pass

    for c in expected_cols:
        if pd.api.types.is_numeric_dtype(out[c]):
            out[c] = out[c].fillna(_infer_default(c))

    return out

def print_schema_diff(df: pd.DataFrame, expected_cols: list[str]):
    produced = set(df.columns)
    expected = set(expected_cols)
    missing = sorted(list(expected - produced))
    extra   = sorted(list(produced - expected))
    print(f"[Schema] Produced: {len(produced)} | Expected: {len(expected)}")
    print(f"[Schema] Missing ({len(missing)}): {missing[:25]}{' ...' if len(missing)>25 else ''}")
    print(f"[Schema] Extra   ({len(extra)}): {extra[:25]}{' ...' if len(extra)>25 else ''}")

if 'df_runtime_only' not in globals():
    raise RuntimeError("df_runtime_only not found in scope.")

expected_cols = load_schema(SCHEMA_PATH)
if expected_cols is None:
    expected_cols = list(df_runtime_only.columns)
    save_schema(expected_cols, SCHEMA_PATH)
    print(f"[Registry] Created at {SCHEMA_PATH} with {len(expected_cols)} columns.")

print_schema_diff(df_runtime_only, expected_cols)

df_runtime_only_locked = enforce_schema(df_runtime_only, expected_cols, drop_extras=DROP_EXTRAS)

print_schema_diff(df_runtime_only_locked, expected_cols)
df_runtime_only_locked.to_csv(SAVE_LOCKED_AS, index=False)
print(f"[Saved] Locked dataset -> {SAVE_LOCKED_AS}")

missing_after = set(expected_cols) - set(df_runtime_only_locked.columns)
assert not missing_after, f"Missing columns after enforcement: {missing_after}"

df = pd.read_csv('/content/df_runtime_only_locked.csv')
df.isnull().sum()

"""**MODELING WORK BEGINS**"""

import json
import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import List, Tuple, Dict

from scipy.stats import ks_2samp

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    mean_absolute_error, mean_squared_error
)
from sklearn.linear_model import LogisticRegression, Ridge

CSV_PATH      = "final_df1_holiday_flag_fixed-2 - Sheet1.csv"
DATE_COL      = "last_tx_date"
DOB_COL       = "dob"
SCORE_COL     = "fico8"
ALT_SCORE_COL = "vantage4"

GOOD_CUTOFF   = 670

TRAIN_FRAC = 0.70
VAL_FRAC   = 0.15

df = pd.read_csv(CSV_PATH)
df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors="coerce")
df[DOB_COL]  = pd.to_datetime(df[DOB_COL], errors="coerce")

print("Loaded shape:", df.shape)
print("Date range:", df[DATE_COL].min(), "->", df[DATE_COL].max())
print("Columns:", list(df.columns)[:10], "...")

df["target_good"]  = (df[SCORE_COL].astype(float) >= GOOD_CUTOFF).astype(int)
df["target_score"] = df[SCORE_COL].astype(float)

df["age_years"] = (df[DATE_COL].dt.to_period("M").dt.to_timestamp() - df[DOB_COL]).dt.days / 365.25

df["__period"] = df[DATE_COL].dt.to_period("M")

def build_features_frame(data: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    base_drop = {
        SCORE_COL, ALT_SCORE_COL,
        "good", "target_good", "target_score",
        DOB_COL, DATE_COL, "__period",
        "gender", "customer_id", "category_id"
    }
    regex_patterns = [
        r'(?i)^gender.*',
        r'(?i).*_id$',
        r'(?i)^id$',
        r'(?i).*uuid.*',
        r'(?i).*hash.*',
        r'(?i).*vantage.*',
        r'(?i).*fico.*',
        r'(?i)\bgood\b',
    ]
    drop_cols = set(base_drop)
    for pat in regex_patterns:
        drop_cols |= set(data.filter(regex=pat, axis=1).columns)

    kept = [c for c in data.columns if c not in drop_cols]
    X = data[kept].copy()

    dt_like = []
    for c in list(X.columns):
        if np.issubdtype(X[c].dtype, np.datetime64):
            dt_like.append(c)
    if dt_like:
        X.drop(columns=dt_like, inplace=True)

    removed = sorted(list(set(data.columns) - set(X.columns)))
    return X, removed

X_all, removed_cols = build_features_frame(df)
y_cls = df["target_good"].values
y_reg = df["target_score"].values

print(f"Feature columns: {X_all.shape[1]}")
print("Dropped (for safety):", removed_cols[:20], "..." if len(removed_cols) > 20 else "")

def make_splits_monthly(data: pd.DataFrame, period_col: str, train_frac=0.70, val_frac=0.15):
    months = data[period_col].sort_values().unique()
    n = len(months)
    n_train = int(round(n * train_frac))
    n_val   = int(round(n * val_frac))
    n_test  = n - n_train - n_val

    train_m = months[:n_train]
    val_m   = months[n_train:n_train+n_val]
    test_m  = months[n_train+n_val:]

    def mask(months_allowed):
        allowed = set([str(m) for m in months_allowed])
        return data[period_col].astype(str).isin(allowed).values

    train_mask = mask(train_m)
    val_mask   = mask(val_m)
    test_mask  = mask(test_m)

    return train_mask, val_mask, test_mask, (train_m, val_m, test_m)

train_mask, val_mask, test_mask, (train_m, val_m, test_m) = make_splits_monthly(df, "__period", TRAIN_FRAC, VAL_FRAC)

print("Months total:", len(df["__period"].unique()))
print("Train months:", str(train_m[0]), "->", str(train_m[-1]))
print("Val months:", list(map(str, val_m)))
print("Test months:", list(map(str, test_m)))

def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    cat_cols = [c for c in X.columns if X[c].dtype == "object"]
    num_cols = [c for c in X.columns if X[c].dtype != "object"]

    numeric = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler",  StandardScaler(with_mean=False)),
    ])
    categorical = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot",  OneHotEncoder(handle_unknown="ignore")),
    ])

    return ColumnTransformer([
        ("num", numeric, num_cols),
        ("cat", categorical, cat_cols),
    ])

preprocessor = build_preprocessor(X_all)
print("Preprocessor ready.")

import json
import numpy as np
from scipy.stats import ks_2samp

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    brier_score_loss,
    mean_absolute_error,
    mean_squared_error,
)

def ks_stat(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    pos = y_prob[y_true == 1]
    neg = y_prob[y_true == 0]
    return float(ks_2samp(pos, neg)[0]) if len(pos) and len(neg) else np.nan

clf = Pipeline([
    ("prep", preprocessor),
    ("model", LogisticRegression(max_iter=800, class_weight="balanced", random_state=42)),
])

clf.fit(X_all[train_mask], y_cls[train_mask])
val_probs = clf.predict_proba(X_all[val_mask])[:, 1]
val_cls_metrics = {
    "roc_auc": float(roc_auc_score(y_cls[val_mask], val_probs)),
    "pr_auc":  float(average_precision_score(y_cls[val_mask], val_probs)),
    "brier":   float(brier_score_loss(y_cls[val_mask], val_probs)),
    "ks":      ks_stat(y_cls[val_mask], val_probs),
}

trainval_mask = np.logical_or(train_mask, val_mask)
clf.fit(X_all[trainval_mask], y_cls[trainval_mask])
test_probs = clf.predict_proba(X_all[test_mask])[:, 1]
test_cls_metrics = {
    "roc_auc": float(roc_auc_score(y_cls[test_mask], test_probs)),
    "pr_auc":  float(average_precision_score(y_cls[test_mask], test_probs)),
    "brier":   float(brier_score_loss(y_cls[test_mask], test_probs)),
    "ks":      ks_stat(y_cls[test_mask], test_probs),
}

print("\nClassification (good >= 670):")
print("Validation:", json.dumps(val_cls_metrics, indent=2))
print("Test:",       json.dumps(test_cls_metrics, indent=2))

reg = Pipeline([
    ("prep", preprocessor),
    ("model", Ridge(alpha=1.0, random_state=42)),
])

reg.fit(X_all[train_mask], y_reg[train_mask])
val_pred = reg.predict(X_all[val_mask])
val_reg_metrics = {
    "mae":  float(mean_absolute_error(y_reg[val_mask], val_pred)),
    "rmse": float(np.sqrt(mean_squared_error(y_reg[val_mask], val_pred))),
}

reg.fit(X_all[trainval_mask], y_reg[trainval_mask])
test_pred = reg.predict(X_all[test_mask])
test_reg_metrics = {
    "mae":  float(mean_absolute_error(y_reg[test_mask], test_pred)),
    "rmse": float(np.sqrt(mean_squared_error(y_reg[test_mask], test_pred))),
}

print("\nRegression (predict FICO8):")
print("Validation:", json.dumps(val_reg_metrics, indent=2))
print("Test:",       json.dumps(test_reg_metrics, indent=2))

import numpy as np
import json
from dataclasses import dataclass
from typing import List, Dict
from scipy.stats import ks_2samp
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, mean_absolute_error, mean_squared_error
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor

RANDOM_STATE = 42
N_JOBS = -1
USE_CALIBRATION = True
MIN_TRAIN_MONTHS = 6

@dataclass
class ClassifMetrics:
    roc_auc: float
    pr_auc: float
    brier: float
    ks: float

@dataclass
class RegrMetrics:
    mae: float
    rmse: float

def ks_stat(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    pos = y_prob[y_true == 1]
    neg = y_prob[y_true == 0]
    return float(ks_2samp(pos, neg)[0]) if len(pos) and len(neg) else np.nan

def mask_by_months(period_series, months: List[str]) -> np.ndarray:
    allowed = set(map(str, months))
    return period_series.astype(str).isin(allowed).values

def expanding_window_folds(all_months: List[str], min_train_months: int):
    for i in range(min_train_months, len(all_months)):
        yield all_months[:i], all_months[i]

def eval_classifier(y_true, probs) -> ClassifMetrics:
    return ClassifMetrics(
        roc_auc=float(roc_auc_score(y_true, probs)) if len(np.unique(y_true)) > 1 else np.nan,
        pr_auc=float(average_precision_score(y_true, probs)) if len(np.unique(y_true)) > 1 else np.nan,
        brier=float(brier_score_loss(y_true, probs)),
        ks=ks_stat(y_true, probs),
    )

def eval_regression(y_true, pred) -> RegrMetrics:
    return RegrMetrics(
        mae=float(mean_absolute_error(y_true, pred)),
        rmse=float(np.sqrt(mean_squared_error(y_true, pred))),
    )

def summarize_cls(scores: List[ClassifMetrics]) -> Dict[str, float]:
    return {
        "brier_mean": float(np.mean([s.brier for s in scores])) if scores else float("inf"),
        "roc_mean":   float(np.mean([s.roc_auc for s in scores])) if scores else float("-inf"),
        "pr_mean":    float(np.mean([s.pr_auc for s in scores])) if scores else float("-inf"),
        "ks_mean":    float(np.mean([s.ks for s in scores])) if scores else float("-inf"),
    }

def summarize_reg(scores: List[RegrMetrics]) -> Dict[str, float]:
    return {
        "rmse_mean": float(np.mean([s.rmse for s in scores])) if scores else float("inf"),
        "mae_mean":  float(np.mean([s.mae  for s in scores])) if scores else float("inf"),
    }

CLASSIFIERS = {
    "logit": LogisticRegression(max_iter=800, class_weight="balanced", random_state=RANDOM_STATE),
    "rf":    RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=N_JOBS, class_weight="balanced"),
    "gbrt":  GradientBoostingClassifier(random_state=RANDOM_STATE),
}
REGRESSORS = {
    "ridge": Ridge(alpha=1.0, random_state=RANDOM_STATE),
    "rf":    RandomForestRegressor(n_estimators=400, random_state=RANDOM_STATE, n_jobs=N_JOBS),
    "gbr":   GradientBoostingRegressor(random_state=RANDOM_STATE),
}

all_months = sorted(list(map(str, pd.Series(df["__period"]).unique())))
cv_folds = list(expanding_window_folds(all_months, MIN_TRAIN_MONTHS))

cls_cv_scores = {name: [] for name in CLASSIFIERS}
for name, est in CLASSIFIERS.items():
    for train_m, val_m in cv_folds:
        tr_mask = mask_by_months(df["__period"], train_m)
        va_mask = mask_by_months(df["__period"], [val_m])
        pipe = Pipeline([("prep", preprocessor), ("model", est)])
        pipe.fit(X_all[tr_mask], y_cls[tr_mask])
        probs = pipe.predict_proba(X_all[va_mask])[:, 1]
        cls_cv_scores[name].append(eval_classifier(y_cls[va_mask], probs))

cls_summaries = {k: summarize_cls(v) for k, v in cls_cv_scores.items()}
best_cls = sorted(cls_summaries.items(), key=lambda kv: (kv[1]["brier_mean"], -kv[1]["roc_mean"]))[0][0]

print("[Classification] CV means:")
for k, v in cls_summaries.items():
    print(k, v)
print("Chosen classifier:", best_cls)

reg_cv_scores = {name: [] for name in REGRESSORS}
for name, est in REGRESSORS.items():
    for train_m, val_m in cv_folds:
        tr_mask = mask_by_months(df["__period"], train_m)
        va_mask = mask_by_months(df["__period"], [val_m])
        pipe = Pipeline([("prep", preprocessor), ("model", est)])
        pipe.fit(X_all[tr_mask], y_reg[tr_mask])
        pred = pipe.predict(X_all[va_mask])
        reg_cv_scores[name].append(eval_regression(y_reg[va_mask], pred))

reg_summaries = {k: summarize_reg(v) for k, v in reg_cv_scores.items()}
best_reg = sorted(reg_summaries.items(), key=lambda kv: (kv[1]["rmse_mean"], kv[1]["mae_mean"]))[0][0]

print("\n[Regression] CV means:")
for k, v in reg_summaries.items():
    print(k, v)
print("Chosen regressor:", best_reg)

trainval_mask = np.logical_or(train_mask, val_mask)

final_cls_pipe = Pipeline([("prep", preprocessor), ("model", CLASSIFIERS[best_cls])])
final_cls_pipe.fit(X_all[trainval_mask], y_cls[trainval_mask])
if USE_CALIBRATION:
    final_cls_pipe = CalibratedClassifierCV(final_cls_pipe, method="sigmoid", cv="prefit")
    final_cls_pipe.fit(X_all[trainval_mask], y_cls[trainval_mask])

val_probs = final_cls_pipe.predict_proba(X_all[val_mask])[:, 1]
test_probs = final_cls_pipe.predict_proba(X_all[test_mask])[:, 1]
val_cls = eval_classifier(y_cls[val_mask], val_probs)
test_cls = eval_classifier(y_cls[test_mask], test_probs)

print("\n[Classification] Validation:", val_cls)
print("[Classification] Test:", test_cls)

final_reg_pipe = Pipeline([("prep", preprocessor), ("model", REGRESSORS[best_reg])])
final_reg_pipe.fit(X_all[trainval_mask], y_reg[trainval_mask])

val_pred = final_reg_pipe.predict(X_all[val_mask])
test_pred = final_reg_pipe.predict(X_all[test_mask])
val_reg = eval_regression(y_reg[val_mask], val_pred)
test_reg = eval_regression(y_reg[test_mask], test_pred)

print("\n[Regression] Validation:", val_reg)
print("[Regression] Test:", test_reg)

import numpy as np
import pandas as pd
from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score

fpr, tpr, roc_th = roc_curve(y_cls[val_mask], val_probs)
youden = tpr - fpr
t_roc = roc_th[np.argmax(youden)]

prec, rec, pr_th = precision_recall_curve(y_cls[val_mask], val_probs)
f1s = 2 * (prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)
t_pr = pr_th[np.argmax(f1s)]

t_final = float(t_pr)

y_test_hat = (test_probs >= t_final).astype(int)
cm = confusion_matrix(y_cls[test_mask], y_test_hat)
acc = accuracy_score(y_cls[test_mask], y_test_hat)
prec_ = precision_score(y_cls[test_mask], y_test_hat, zero_division=0)
rec_ = recall_score(y_cls[test_mask], y_test_hat, zero_division=0)
f1_ = f1_score(y_cls[test_mask], y_test_hat, zero_division=0)

print("Threshold_final:", t_final)
print("ConfusionMatrix:\n", cm)
print({"accuracy":acc,"precision":prec_,"recall":rec_,"f1":f1_})

probs = pd.Series(test_probs, name="p")
y = pd.Series(y_cls[test_mask], name="y")
dfd = pd.DataFrame({"p": probs, "y": y})
dfd["decile"] = pd.qcut(dfd["p"].rank(method="first"), 10, labels=list(range(10,0,-1)))
tab = dfd.groupby("decile").agg(n=("y","size"), positives=("y","sum"), rate=("y","mean")).sort_index(ascending=False)
tab["cum_pos"] = tab["positives"].cumsum()
tab["cum_rate"] = tab["cum_pos"] / tab["positives"].sum()
print(tab)

ks_table = dfd.copy()
ks_table["bucket"] = pd.qcut(ks_table["p"], 10, duplicates="drop")
g = ks_table.groupby("bucket")["y"].agg(["size","sum"])
g["cum_good"] = (g["size"] - g["sum"]).cumsum() / (g["size"].sum() - g["sum"].sum())
g["cum_bad"]  = g["sum"].cumsum() / g["sum"].sum()
g["ks"] = (g["cum_bad"] - g["cum_good"]).abs()
print("KS_by_decile_max:", float(g["ks"].max()))

import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss
from scipy.stats import ks_2samp

months_test = sorted(list(map(str, pd.Series(df.loc[test_mask, "__period"]).unique())))
rows = []
for m in months_test:
    tm = (df.loc[test_mask, "__period"].astype(str).values == m)
    pm = test_probs[tm]
    ym = y_cls[test_mask][tm]
    if pm.size == 0:
        continue
    auc = float(roc_auc_score(ym, pm)) if len(np.unique(ym))>1 else np.nan
    pr  = float(average_precision_score(ym, pm)) if len(np.unique(ym))>1 else np.nan
    brier = float(brier_score_loss(ym, pm))
    pos = pm[ym==1]; neg = pm[ym==0]
    ks = float(ks_2samp(pos, neg)[0]) if len(pos) and len(neg) else np.nan
    rows.append({"month":m,"auc":auc,"pr_auc":pr,"brier":brier,"ks":ks,"n":int(tm.sum())})
stab = pd.DataFrame(rows).sort_values("month")
print(stab)

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss

cls_models = {
    "logit": LogisticRegression(max_iter=800, class_weight="balanced", random_state=42),
    "rf":    RandomForestClassifier(n_estimators=150, max_depth=12, min_samples_leaf=5, class_weight="balanced", random_state=42, n_jobs=-1),
    "gbrt":  GradientBoostingClassifier(n_estimators=150, max_depth=3, random_state=42),
}
reg_models = {
    "ridge": Ridge(alpha=1.0, random_state=42),
    "rf":    RandomForestRegressor(n_estimators=200, max_depth=12, min_samples_leaf=5, random_state=42, n_jobs=-1),
    "gbr":   GradientBoostingRegressor(n_estimators=200, max_depth=3, random_state=42),
}

tv = np.logical_or(train_mask, val_mask)

print("\nClassifier_baselines_test:")
for name, est in cls_models.items():
    pipe = Pipeline([("prep", preprocessor), ("model", est)])
    pipe.fit(X_all[tv], y_cls[tv])
    p = pipe.predict_proba(X_all[test_mask])[:,1]
    auc = float(roc_auc_score(y_cls[test_mask], p))
    pr  = float(average_precision_score(y_cls[test_mask], p))
    brier = float(brier_score_loss(y_cls[test_mask], p))
    pos = p[y_cls[test_mask]==1]; neg = p[y_cls[test_mask]==0]
    ks = float(ks_2samp(pos, neg)[0]) if len(pos) and len(neg) else np.nan
    print(name, {"roc_auc":auc,"pr_auc":pr,"brier":brier,"ks":ks})

print("\nRegressor_baselines_test:")
for name, est in reg_models.items():
    pipe = Pipeline([("prep", preprocessor), ("model", est)])
    pipe.fit(X_all[tv], y_reg[tv])
    pred = pipe.predict(X_all[test_mask])
    mae = float(np.mean(np.abs(y_reg[test_mask]-pred)))
    rmse = float(np.sqrt(np.mean((y_reg[test_mask]-pred)**2)))
    print(name, {"mae":mae,"rmse":rmse})

import numpy as np
import pandas as pd

months_test = sorted(list(map(str, pd.Series(df.loc[test_mask, "__period"]).unique())))

y_test   = y_reg[test_mask]
pred_test= test_pred
per_test = df.loc[test_mask, "__period"].astype(str).values

rows = []
for m in months_test:
    tm = (per_test == m)
    if not np.any(tm):
        continue
    ym = y_test[tm]
    pm = pred_test[tm]
    mae = float(np.mean(np.abs(ym - pm)))
    rmse = float(np.sqrt(np.mean((ym - pm) ** 2)))
    rows.append({"month": m, "mae": mae, "rmse": rmse, "n": int(tm.sum())})

print(pd.DataFrame(rows).sort_values("month"))

res = y_test - pred_test
print({
    "residual_mean": float(np.mean(res)),
    "residual_std":  float(np.std(res)),
    "p10":           float(np.percentile(res, 10)),
    "p50":           float(np.percentile(res, 50)),
    "p90":           float(np.percentile(res, 90)),
})

import numpy as np, pandas as pd
from sklearn.metrics import roc_curve, precision_recall_curve

fpr, tpr, roc_th = roc_curve(y_cls[val_mask], val_probs)
youden = tpr - fpr
t_roc = roc_th[np.argmax(youden)]

prec, rec, pr_th = precision_recall_curve(y_cls[val_mask], val_probs)
f1s = 2 * (prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)
t_pr = pr_th[np.argmax(f1s)]

t_final = float(t_pr)

id_col = "customer_id" if "customer_id" in df.columns else None

p_test = pd.Series(test_probs, index=df.index[test_mask], name="p_good")
s_test = pd.Series(test_pred,  index=df.index[test_mask], name="fico_pred")
per_test = df.loc[test_mask, "__period"].astype(str)
out_test = pd.DataFrame({"period": per_test, "p_good": p_test, "fico_pred": s_test})
if id_col: out_test.insert(0, id_col, df.loc[test_mask, id_col].values)
out_test["decision"] = (out_test["p_good"] >= t_final).astype(int)
out_test["decile"] = pd.qcut(out_test["p_good"].rank(method="first"), 10, labels=list(range(10,0,-1)))
print("\nTEST PREDICTIONS (head)")
print(out_test.head(10).to_string(index=False))

latest_month = str(sorted(out_test["period"].unique())[-1])
mask_latest = out_test["period"] == latest_month
out_latest = out_test.loc[mask_latest].copy()
print(f"\nLATEST MONTH PREDICTIONS [{latest_month}] (head)")
print(out_latest.head(10).to_string(index=False))

